% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/analyze_tokens.R
\name{summarize_tokens}
\alias{summarize_tokens}
\title{Summarize Tokenized Biological Sequences}
\usage{
summarize_tokens(tokens)
}
\arguments{
\item{tokens}{A non-empty list of character vectors representing tokenized
sequences.}
}
\value{
An object of class \code{"bioBPE_summary"}, being a list containing:
\describe{
\item{\code{corpus}}{List of corpus-level statistics: number of sequences,
total tokens, average and median sequence lengths, and vocabulary
size.}
\item{\code{token_summary}}{Data frame with each token's frequency, length,
and GC-like flag.}
\item{\code{token_length_summary}}{Summary of token lengths.}
}
}
\description{
This function computes summary statistics for a list of tokenized biological
sequences, including corpus-level metrics, token frequencies, token lengths,
and basic biological features (e.g., GC-like tokens). These analysis metrics
are further used for downstream analysis and visualization by the pipeline.
}
\examples{
\dontrun{
   # Generate simulated data
   data <- generate_data(
       n          = 3, 
       length     = 1000, 
       vocab_size = 25, 
       preprocess = TRUE,
       annotate   = TRUE,
       tokenize   = TRUE,
       summarize  = FALSE,
       verbose    = FALSE
   )
   
   # Summarize the preprocessed, annotated, and tokenized sequences
   dna_summary <- summarize_tokens(tokens = data$dna_tokens$tokens)
   rna_summary <- summarize_tokens(tokens = data$rna_tokens$tokens)
   aa_summary <- summarize_tokens(tokens = data$aa_tokens$tokens)
}

}
\references{
{
R Core Team (2025). \emph{R: A Language and Environment for Statistical
Computing}. R Foundation for Statistical Computing, Vienna, Austria.
\url{https://www.R-project.org/}.

Dotan E, Jaschek G, Pupko T, Belinkov Y (2024). Effect of tokenization on
transformers for biological sequences. Bioinformatics, 40(4): btae196.
doi:10.1093/bioinformatics/btae196. PMCID: PMC11055402.
}
}
\concept{summary}
\keyword{summary}
